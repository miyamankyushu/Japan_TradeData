# æ—¥æœ¬è²¿æ˜“çµ±è¨ˆãƒ‡ãƒ¼ã‚¿è‡ªå‹•å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆHScodeï¼‰ ğŸŒ
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€æ—¥æœ¬ã®æ”¿åºœçµ±è¨ˆï¼ˆ[e-Stat API](https://www.e-stat.go.jp/stat-search/files?page=1&toukei=00350300&tstat=000001013143)ï¼‰ãŠã‚ˆã³ç¨é–¢ã®è¼¸å‡ºçµ±è¨ˆå“ç›®è¡¨([HP](https://www.customs.go.jp/yusyutu/index.htm))
ã‚’æ´»ç”¨ã—ã€HSã‚³ãƒ¼ãƒ‰åˆ¥ã®è²¿æ˜“çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ã§åé›†ãƒ»åŠ å·¥ãƒ»ä¿å­˜ã™ã‚‹ãŸã‚ã®Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆç¾¤ã§ã™ã€‚]



---

## ğŸ” ä¸»ãªç‰¹å¾´

- HSã‚³ãƒ¼ãƒ‰ï¼ˆå“ç›®åˆ†é¡ï¼‰ã®éšå±¤ãƒã‚¹ã‚¿è‡ªå‹•ç”Ÿæˆï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
- e-Stat API ã‹ã‚‰ã®æœˆæ¬¡è²¿æ˜“çµ±è¨ˆãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæŒ‡å®šã•ã‚ŒãŸæœˆãƒ»å¹´ãƒ»åˆ†é¡ã§ï¼‰
- éƒ¨é¡/é¡/é …ç›®ãƒ¬ãƒ™ãƒ«ã§ã®ãƒã‚¹ã‚¿çµ±åˆ
- å›½åˆ¥ãƒ»ç¨é–¢åˆ¥ã«åˆ†é¡ã•ã‚ŒãŸå–å¼•ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§å‡ºåŠ›
- å“è³ªæ¤œæŸ»ï¼ˆæ¬ æå€¤ã€æ§‹é€ æ¬ è½ã€æ¡æ•°ãƒã‚§ãƒƒã‚¯ï¼‰ä»˜ã

---

## ğŸ“Š ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ

```
JAPAN_TRADEDATA/
â”œâ”€â”€ library/                          # Pythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¾¤
â”‚   â”œâ”€â”€ get_export_data_HSitem.py     # TradeDataPipelineã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ hscode_scrape.py              # HSã‚³ãƒ¼ãƒ‰å–å¾—ãƒ­ã‚¸ãƒƒã‚¯
â”œâ”€â”€ reference_master/                 # ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿é¡
â”‚   â”œâ”€â”€ area/nation.xlsx              # å›½ãƒ»åœ°åŸŸãƒã‚¹ã‚¿
â”‚   â”œâ”€â”€ counter/è²¿æ˜“çµ±è¨ˆ_å¯¾å¿œè¡¨.xlsx   # APIæƒ…å ±å¯¾å¿œè¡¨
â”‚   â”œâ”€â”€ HS_master/                    # HSã‚³ãƒ¼ãƒ‰ãƒã‚¹ã‚¿ã¨ãƒ­ã‚°
â”‚   â””â”€â”€ e-stat/month.json             # æœˆåˆ¥ã‚³ãƒ¼ãƒ‰å®šç¾©
â”œâ”€â”€ Output/                           # å‡ºåŠ›ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ MIT License
â”œâ”€â”€ requirment
â””â”€â”€ HSCode_scraping.ipynb             # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
```

---
## ğŸ›  ä½¿ç”¨æ–¹æ³•

### 1. HSã‚³ãƒ¼ãƒ‰ãƒã‚¹ã‚¿ã®ç”Ÿæˆï¼ˆ`HScode_scrape.py`ï¼‰
```python
from library.hscode_scrape import (generate_customs_urls,fetch_and_concat_data,validate_and_log_hs_dataframe)
#å¹´ãƒ»æœˆãƒ»éƒ¨é¡ç•ªå·ã®æŒ‡å®š
year, month = 2010, 1    # å¹´ãƒ»æœˆï¼ˆâ€»ç¯„å›²æŒ‡å®šï¼‰
num_range = range(1, 98) # éƒ¨é¡ç•ªå·ï¼ˆâ€»ç¯„å›²æŒ‡å®šï¼‰
# HSã‚³ãƒ¼ãƒ‰ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
urls = generate_customs_urls(year, month, num_range)
df = fetch_and_concat_data(urls)
log_df = validate_and_log_hs_dataframe(df, year) #ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³logãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
# CSVã§ä¿å­˜
df.to_csv(f'./reference_master/HS_master/HSã‚³ãƒ¼ãƒ‰ãƒã‚¹ã‚¿_{year:04d}.csv', encoding='utf-8', index=False)
```

### 2. è²¿æ˜“çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆget_export_data_HSitem.pyï¼‰
æ³¨æ„äº‹é …ï¼š APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™
e-Stat API(https://www.e-stat.go.jp/api)ã®åˆ©ç”¨ç”³è«‹ã‚’è¡Œã„ã€ã”è‡ªèº«ã®`YOUR_API_KEY`ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚
```python
from library.get_export_data_HSitem import TradeDataPipeline
import pandas as pd
#å¹´ãƒ»æœˆãƒ»éƒ¨é¡ç•ªå·ã®æŒ‡å®š
syurui = 'HS'  # HSå“ç›®åˆ¥
year, month = 2024, '01' # å¹´ãƒ»æœˆï¼ˆâ€»å˜ä¸€æŒ‡å®šï¼‰
api_key = "***************" #ã”è‡ªèº«ã§å–å¾—ã—ã¦ãã ã•ã„

# è²¿æ˜“çµ±è¨ˆãƒã‚¹ã‚¿ã¨å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
trade_counter_df = pd.read_excel('./reference_master/counter/è²¿æ˜“çµ±è¨ˆ_å¯¾å¿œè¡¨.xlsx', dtype=str)
hs_counter_df = trade_counter_df[(trade_counter_df['åˆ†é¡'] == syurui) & (trade_counter_df['year'].astype(int) == year)].reset_index(drop=True)

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
pipeline = TradeDataPipeline(hs_counter_df, trade_counter_df, nation_df, month, api_key)
pipeline.run()
```
---
## ğŸ’¾ å‡ºåŠ›çµæœ
å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ï¼š./Output/HS_item/

å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åä¾‹ï¼š
2024_01_ç¨é–¢åˆ¥_20250423.csv

ã‚«ãƒ©ãƒ ä¾‹ï¼š
```bash
['åœ°åŸŸ', 'å›½', 'å¹´', 'æœˆ', 'ç¨é–¢', 'éƒ¨æ•°', 'éƒ¨å', 'é¡æ•°', 'é¡å', 'HSã‚³ãƒ¼ãƒ‰',
 'å¤§é …ç›®', 'ä¸­é …ç›®', 'å°é …ç›®', 'ç´°é …ç›®', 'å¾®ç´°é …ç›®', 'é …ç›®', 'é‡‘é¡', 'é‡‘é¡å˜ä½', 'æ•°é‡', 'æ•°é‡å˜ä½']
```
---
## âš ï¸ æ³¨æ„äº‹é …

- APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ï¼še-Stat APIï¼ˆhttps://www.e-stat.go.jp/apiï¼‰ã®åˆ©ç”¨ç”³è«‹ã‚’è¡Œã„ã€`YOUR_API_KEY`ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚
- reference_master/ ä»¥ä¸‹ã®CSVãŒå¿…è¦ã§ã™ï¼šåˆ†é¡ã€ãƒã‚¹ã‚¿ã€å›½åå¤‰æ›ãªã©ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«æ ¼ç´ã•ã‚ŒãŸCSVã‚’å‰æã¨ã—ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚‰ã¯ç§ãŒä½œæˆã—ã¦ã„ã‚‹ã‚‚ã®ã§ã™ã€‚é©å®œã”å¤‰æ›´ãã ã•ã„ã€‚
- å¹´ãƒ»æœˆãƒ»åˆ†é¡ã«ã‚ˆã£ã¦ã¯å¯¾å¿œã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆç¨é–¢ã®HTMLæ§‹é€ å¤‰åŒ–ã‚„APIä»•æ§˜ã«ã‚ˆã‚‹ï¼‰ã€‚



---
## ğŸ“ ä½œæˆè€…
Hiroki Watariï¼ˆæ¸¡åˆ© åºƒå¸Œï¼‰
ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ / ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å€‹äººãƒ»ä¼æ¥­å•ã‚ãšè‡ªç”±ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ã§ã™ã€‚
Forkã‚„Issueæ­“è¿ã—ã¾ã™ï¼


# Japan Trade Statistics Auto-Collection Pipeline ğŸŒ
This project is a set of Python scripts for automatically collecting, processing, and saving Japanâ€™s trade statistics data by HS code. It utilizes the **e-Stat API** (provided by the Japanese government) and **web scraping of Japan Customs pages**.



---

## ğŸ” Key Features

- Automatically generates a hierarchical master of HS codes (via web scraping)
- Retrieves monthly trade statistics from the e-Stat API (by year/month/category)
- Merges and enriches data with HS classification hierarchy
- Outputs transaction data classified by country and customs office in CSV format
- Includes data validation features (missing values, structure gaps, digit checks)

---

## ğŸ“Š Project Structure

```
JAPAN_TRADEDATA/
â”œâ”€â”€ library/                          # Pythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¾¤
â”‚   â”œâ”€â”€ get_export_data_HSitem.py     # TradeDataPipelineã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ HScode_scrape.py              # HSã‚³ãƒ¼ãƒ‰å–å¾—ãƒ­ã‚¸ãƒƒã‚¯
â”œâ”€â”€ reference_master/                # ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿é¡
â”‚   â”œâ”€â”€ area/nation.xlsx              # å›½ãƒ»åœ°åŸŸãƒã‚¹ã‚¿
â”‚   â”œâ”€â”€ counter/è²¿æ˜“çµ±è¨ˆ_å¯¾å¿œè¡¨.xlsx     # APIæƒ…å ±å¯¾å¿œè¡¨
â”‚   â”œâ”€â”€ HS_master/                    # HSã‚³ãƒ¼ãƒ‰ãƒã‚¹ã‚¿ã¨ãƒ­ã‚°
â”‚   â””â”€â”€ cat02_master.json            # æœˆåˆ¥ã‚³ãƒ¼ãƒ‰å®šç¾©
â”œâ”€â”€ Output/                          # å‡ºåŠ›ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ debug/                           # ä¸€æ™‚çš„ãªæ¤œè¨¼å‡ºåŠ›
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ HSCode_scraping.ipynb            # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
```

---
## ğŸ›  How to Use

### 1. Generate the HS Code Master (`HScode_scrape.py`)
```python
from library.HScode_scrape import generate_customs_urls, fetch_and_concat_data, validate_and_log_hs_dataframe

urls = generate_customs_urls(2024, 1, range(1, 98))  Specify year, month, and section range
df = fetch_and_concat_data(urls)
validate_and_log_hs_dataframe(df, 2024)
df.to_csv('./reference_master/HS_master/HSã‚³ãƒ¼ãƒ‰ãƒã‚¹ã‚¿_2024.csv', index=False, encoding='utf-8')
```

### 2. Retrieve Trade Statistics Data (get_export_data_HSitem.py)
```python
from library.get_export_data_HSitem import TradeDataPipeline
import pandas as pd

# Load master files
hs_df = pd.read_csv('...')  # HS classification settings
stat_df = pd.read_csv('...')  # e-Stat statID mapping
nation_df = pd.read_csv('...')  # Country name mapping

pipeline = TradeDataPipeline(hs_df, stat_df, nation_df, '01', 'YOUR_API_KEY')
pipeline.run()
```
---
## ğŸ’¾ Output
Output folder: ./Output/HS_item/

Example output filename: 2024_01_by_customs_20250423.csv

Sample columns:
```bash
['Region', 'Country', 'Year', 'Month', 'Customs', 'Section No', 'Section Name',
 'Category No', 'Category Name', 'HS Code', 'Main Item', 'Sub Item 1', 'Sub Item 2',
 'Sub Item 3', 'Sub Item 4', 'Item', 'Amount', 'Amount Unit', 'Quantity', 'Quantity Unit']

```
---
## âš ï¸ Notes

API key is required
You must apply for and obtain an API key from the e-Stat API.

You need CSV files under reference_master/
These include classification definitions, statID mappings, and country mappings.
These are custom-created and may need to be adjusted for your use.

Some years or classifications may not be supported
This is due to changes in HTML structure of customs pages or limitations of the e-Stat API.



---
## ğŸ“ Author
Hiroki Watari
Data Scientist / Data Engineer

This project is open for customization and free to use for both personal and business purposes.
Forks, Issues, and Pull Requests are very welcome!
